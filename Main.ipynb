{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c881f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "\n",
    "from environment import BaseEnvironment\n",
    "from lunar_lander import LunarLanderEnvironment\n",
    "from agent import BaseAgent \n",
    "\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import shutil\n",
    "from plot_script import plot_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5ddb28",
   "metadata": {},
   "source": [
    "## Neural Network for action values\n",
    "\n",
    "We use a neural network with one hidden layer for approximating the action-value function in a control problem. The output layer size is the number of actions. \n",
    "\n",
    "The get_action_values() function computes the action-value function by doing a forward pass.\n",
    "The get_TD_update() function computes the gradient of the action-value function with respect to the weights times the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e83475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
    "        \n",
    "        self.layer_sizes = [self.state_dim, self.num_hidden_units, self.num_actions]\n",
    "        \n",
    "        # Initialize the weights of the neural network\n",
    "        # self.weights is an array of dictionaries with each dictionary corresponding to \n",
    "        # the weights from one layer to the next. Each dictionary includes W and b\n",
    "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "    \n",
    "\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        \n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        q_vals = np.dot(x, W1) + b1\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "\n",
    "    def get_TD_update(self, s, delta_mat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "            correspond to one state in the batch. Each row has only one non-zero element \n",
    "            which is the TD-error corresponding to the action taken.\n",
    "        Returns:\n",
    "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "        \"\"\"\n",
    "\n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        dx = (psi > 0).astype(float)\n",
    "\n",
    "        # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "        # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "        # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "        td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "        v = delta_mat\n",
    "        td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "        td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "        v = np.dot(v, W1.T) * dx\n",
    "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "        td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "        return td_update\n",
    "    \n",
    "\n",
    "    def init_saxe(self, rows, cols):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rows (int): number of input units for layer.\n",
    "            cols (int): number of output units for layer.\n",
    "        Returns:\n",
    "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
    "        \"\"\"\n",
    "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        tensor, r = np.linalg.qr(tensor)\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        tensor *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        return tensor\n",
    "    \n",
    "  \n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns: \n",
    "            A copy of the current weights of this network.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.weights)\n",
    "    \n",
    "  \n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
    "        \"\"\"\n",
    "        self.weights = deepcopy(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d168687",
   "metadata": {},
   "source": [
    "## Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691cf733",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    \n",
    "    def __init__(self, layer_sizes, \n",
    "                 optimizer_info):\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        # Specify Adam algorithm's hyper parameters\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "        \n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            \n",
    "            \n",
    "            self.m[i][\"W\"] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            self.m[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))\n",
    "            self.v[i][\"W\"] = np.zeros((self.layer_sizes[i],self.layer_sizes[i+1]))\n",
    "            self.v[i][\"b\"] = np.zeros((1, self.layer_sizes[i+1]))\n",
    "        \n",
    "            \n",
    "            \n",
    "        # To calculate m_hat and v_hat, we use powers of beta_m and beta_v to \n",
    "        # the time step t. We can calculate these powers using an incremental product. At initialization then, \n",
    "        # beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to \n",
    "        # start from 0, the denominator would be 0.)\n",
    "        self.beta_m_product = self.beta_m\n",
    "        self.beta_v_product = self.beta_v\n",
    "    \n",
    "    \n",
    "    def update_weights(self, weights, td_errors_times_gradients):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (Array of dictionaries): The weights of the neural network.\n",
    "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
    "            action-values with respect to the network's weights times the TD-error\n",
    "        Returns:\n",
    "            The updated weights (Array of dictionaries).\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "                \n",
    "         \n",
    "                self.m[i][param] = self.beta_m * self.m[i][param] + (1 - self.beta_m) * td_errors_times_gradients[i][param]\n",
    "                self.v[i][param] = self.beta_v * self.v[i][param] + (1 - self.beta_v) * td_errors_times_gradients[i][param] ** 2\n",
    "                 \n",
    "                m_hat = self.m[i][param]/(1-self.beta_m_product)\n",
    "                v_hat = self.v[i][param]/(1-self.beta_v_product)\n",
    "                weight_update = self.step_size / (np.sqrt(v_hat) + self.epsilon) * m_hat\n",
    "                \n",
    "               \n",
    "                \n",
    "                weights[i][param] = weights[i][param] + weight_update\n",
    "        # To calculate m_hat and v_hat, we use powers of beta_m and beta_v to \n",
    "        ### update self.beta_m_product and self.beta_v_product\n",
    "        self.beta_m_product *= self.beta_m\n",
    "        self.beta_v_product *= self.beta_v\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59e3e5",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1447670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25a8337",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb87db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values/tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences, axis=1)\n",
    "        \n",
    "    \n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences - reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences, axis=1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences/reshaped_sum_of_exp_preferences\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0635bd91",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a50cccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Q(t+1)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    " \n",
    "    \n",
    "    \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    # Pi\n",
    "    probs_mat = softmax(q_next_mat, tau)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # v_next_vec is a 1D array of shape (batch_size,)\n",
    "\n",
    "    v_next_vec = np.sum(probs_mat * q_next_mat, axis=1) * (1-terminals)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    # \n",
    "    target_vec = rewards + discount*v_next_vec\n",
    "\n",
    "    \n",
    "    \n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "   \n",
    "    q_mat = network.get_action_values(states)\n",
    "   \n",
    "    \n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    q_vec = q_mat[batch_indices, actions]\n",
    "\n",
    "    \n",
    "    \n",
    "    # Compute TD errors for actions taken\n",
    "    # delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    delta_vec = target_vec - q_vec\n",
    "    \n",
    "    \n",
    "    return delta_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43dcf7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # q_vec is a 1D array of shape (batch_size)\n",
    "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch_size - 1. \n",
    "    batch_indices = np.arange(batch_size)\n",
    "\n",
    "    # Make a td error matrix of shape (batch_size, num_actions)\n",
    "    # delta_mat has non-zero value only for actions taken\n",
    "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
    "    delta_mat[batch_indices, actions] = delta_vec\n",
    "\n",
    "    # Pass delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation\n",
    "    td_update = network.get_TD_update(states, delta_mat)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Pass network.get_weights and the td_update to the optimizer to get updated weights\n",
    "    weights = optimizer.update_weights(network.get_weights(), td_update)\n",
    "    \n",
    "    \n",
    "    network.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003c90e3",
   "metadata": {},
   "source": [
    "## Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60c28d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_values, self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        return action\n",
    "\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 0, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network \n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer       \n",
    "       \n",
    "        self.replay_buffer.append(self.last_state, self.last_action, reward, 1, state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network\n",
    "                optimize_network(experiences, self.discount, self.optimizer, self.network, current_q, self.tau)\n",
    "                \n",
    "        \n",
    "    def trained_agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the trained agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        action = self.policy(state)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f250b0",
   "metadata": {},
   "source": [
    "## Run the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abe58618",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1000/1000 [30:57<00:00,  1.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained reward = [[nan  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan  0.  0.  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan nan  0.  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan nan nan  0.  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan nan nan nan  0.  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan nan nan nan nan  0.  0.]]\n",
      "Trained reward = [[nan nan nan nan nan nan nan nan nan  0.]]\n",
      "Trained reward = [[nan nan nan nan nan nan nan nan nan nan]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "    trained_agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes_trained\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "\n",
    "            \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "          \n",
    "        \n",
    "        for episode in range(1, experiment_parameters[\"num_episodes_trained\"]+1):\n",
    "            rl_glue.trained_rl_episode(experiment_parameters[\"timeout\"])\n",
    "            trained_agent_sum_reward[run - 1, episode - 1 ] =  rl_glue.rl_env_message(\"get_sum_reward\")\n",
    "            \n",
    "            print(\"Trained reward = \" + str(trained_agent_sum_reward))\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes_trained\" : 10,\n",
    "    \"num_episodes\" : 1000,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 500.\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = LunarLanderEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 8,\n",
    "        'num_hidden_units': 256,\n",
    "        'num_actions': 4\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "604a3286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGoCAYAAABL+58oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd1yW9frA8c/F3iDIEgXcMycq5fGklamV7TLNyrJ5POe0d52m59ewvY5NbZjtaZmmmWXubS5QEQFliMje398f98MTIAoi8DCu9+vFi+f53t/7vq8bXsDFd4oxBqWUUkqp1sTJ0QEopZRSSjU0TXCUUkop1epogqOUUkqpVkcTHKWUUkq1OprgKKWUUqrV0QRHKaWUUq2OJjhKqUYhIo+KSIaj46gLEUkQkZkOuO/FIrJERLJEpEhEdonIkyLSvqljUaq1cXF0AEop1QxcBBxqyhuKyHPAbcB7wAtANtAHuBnoa4tJKVVPmuAopVodEXEFyo0xZXWpb4zZ0MghVSEiE4A7gGnGmHcrHfpVRN4Ezj7J6zsDzsaY4pO5jlItmXZRKaUcRkQCRWSWiKSKSKGI/CEiw6vVuVNE1ojIEVu970SkW7U6S0XkcxG5UUR2A4VAh4puMhEZJCIrRSRfRDaIyMhq51fpohKR2SKyVkTGiMhmEckTkd9FpG+189qJyDzb8RQRuVdEZopIQi2PfjuwvlpyA4AxpswY86Pt+qNExIhIv5qet4Z4LxSRP23PP9x27jnVznUWkYMi8kSlsn4iMl9Ecmwfn4lIWC3PoFSzpgmOUsohRMQd+BkYA9wNXAikAz9X++PaEXgVuAC4AXAGlouIf7VLjgBuAe4FJgBHbOVewBxgFnAJUAR8JSJetYQYCTwLzAAmASHApyIilerMtsV/K3AjVsvLxFqe2xU4DVhQy/1PVDTwDPB/wDnAXmB1DfGcDoQCn9ji6QYsBzyAq4CpWF1k31V7VqVaFO2iUko5yhSgH9DXGBMHICI/AzuBO7GSHowxt1ecYOt6WQSkYSU871e6XgAwyBhzsFJ9AE/gNmPMElvZAWAD8HeOn2QEAiMqxeYEfAX0BHbYWlXOBy43xnxmq7MY2A/kHue6QYA7kHicOvURBJxljNlYUSAi84BHRcTdGFNkK54IbDPGbLW9fwQ4CIyv6NISkc3ADqxEaX4Dx6lUk9AWHKWUo5wFrAP2ioiLiFT8w/UrEFNRSURiRWSRiBwCSoF8wAfoUe166yonN5WUAEsrvd9m+9yxlvgSKpKbY5xXEeN3FRWMMQVYrVJ10dA7HSdXTm5sPgV8gXEAtq/xxcC8SnXOwkrcyit9H/YCCVT6PijV0miCo5RylPZALFYCUvnjWqATgIhEAgsBAW7C6oYaitWC41HteqnHuE+2Maa84k2lgbfVz68uq9r76ueFATnGmMJq9dJrue4hrG6yyFrqnaijnt8Ykwz8zl/dVGdifd0rJzjtsbr1qn8fumD7PijVEmkXlVLKUTKBtVjjZqqr6E4ZhzWG5gJjTB7YWyECazinoVtEanMQ8BURj2pJTvDxTjLGlIjIcmAs8FAt96i4rlu18kCg+hpDx3r+T4CnRMQTK9HZUK1lKhOrBeftGs5tEesYKVUTbcFRSjnKYqAbkGiMWVvtY4utjidQjtU1VeFymsc/Z2ttn8+vKLAlEWPqcO6LQIyIXFP9gIg4icg429sk2+felY53whoHVFefYX0dL7J9zKt2fDHWWKh1NXwfEk7gPko1K83hl4RSqvVyE5FLayj/FWuA8M3AUtsU7T1YA2WHAQeNMS8AS7BmTb0nIu9gze65i6O7j5qcMWariHwHvCEivlgtOndgjREqr+Xc70TkeeAdERkBfIM1MLkX1tckAVhgjEkSkTXAEyKSj/VP6QNYrS51jTNNRJYCM7EGYn9arcqjWLOt5ovIu1itNhFYidpsY8zSut5LqeZEExylVGPyxWpBqG60MWapiIwGHgcew5q6nIb1x/ZbAGPMFhG5Fmumz0XAJuAybFOcm4GpwBvAy1gJymtYidrQ2k40xtwpIn8A/wTmYrWyJGA9e+VtIyZjdR99iNWicw/WOjonYh7wFrCyequMMWaXiMQCTwJv2uJIxmrZiT/B+yjVbIgxTd1trZRSrZNtfNBWYJUx5qjuJ6VU09EWHKWUqicRuQzoAGwB/LAWIuwOXO3IuJRSOsi4wYmIh4isFpFNIvKniDxmK+8sIqtEJE5EPhERN1u5u+19vO14tCPjV0qdkDysae3fAh9jzaCaYIxZ7dColFLaRdXQbEubextjcm1Lsv+OtYz7HcCXxph5IvI/YJMx5g0R+QfQ3xhzs4hcAVxkjDnuUu9KKaWUOj5twWlgxlKxTLur7cMAZwAVm+PNwdp3B6zl5ufYXn8OnKn7vyillFInR8fgNALbfjnrsNb4eA3YDWQZYyrW8kjCmoaJ7fN+AGNMqYgcwZoqm1HtmjdibeaHt7f3kF69ejX2YyillFLN3rp16zKMMUctsKkJTiMwxpQBA0UkAGuF0N41VbN9rqm15qh+Q2PMm1hTOImJiTFr16496iSllFKqrRGRfTWVaxdVIzLGZGFt8hcLBFTaTLAjkGJ7ncRf++64AP6cwCJeSimllDqaJjgNTESCbS03Fcu2nwVsB34BKlZ0vQZr5VKwZl9UrJdxKbDE6MhvpZRS6qRoF1XDCwfm2MbhOAGfGmO+F5FtwDwReRLYALxjq/8O8IGIxGO13FzhiKCVUkqp1kQTnAZmjNkMDKqhfA/WHjvVywuxlp5XSimlVAPRLiqllFJKtTragtPKZGdnk5aWRklJiaNDUSfI1dWVkJAQ/Pz8HB2KUkq1eJrgtCLZ2dmkpqYSERGBp6cnul5gy2GMoaCggOTkZABNcpRS6iRpF1UrkpaWRkREBF5eXprctDAigpeXFxEREaSlpTk6HKWUavE0wWlFSkpK8PT0dHQY6iR4enpq96JSSjUATXBaGW25adn0+6eUUg1DExyllFJKtTqa4CillFKq1dEERznMihUrGDduHP7+/vj4+DBkyBDmzJnTqPd88sknERHef//9Rr1PTZYuXYqLi05cVEqppqAJjnKIhQsXMnr0aE499VT27NlDWloa9957L7fddhuPPPJIva97vAG65eXlvPPOOwQGBjJr1qx630MppVTzpwmOcojp06czadIkHnnkEYKCgvDy8uLyyy/nhRdeYMaMGSQkJAAwdepUrr/++irnRkdH8+GHHwIwe/ZsunXrxrPPPkvHjh0ZOHDgMe/5008/kZSUxPvvv88ff/zB1q1bqxzftWsXp59+On5+fgwYMICXXnqpyqDf0tJS/vvf/9KjRw8CAgIYMWIE69atsx+fOnUqV111FTfccAMBAQFERETYE6mUlBTGjx9PWVkZPj4++Pj4NHprlVJKtWXaXt7KPfbdn2xLyW6Se/Xp4McjE/rWWm/Xrl3Ex8fzv//976hjkydPZtq0aSxatIgbbrihTvdNSEggJSWFuLg4jrcR+6xZsxg/fjznnnsuAwYM4M033+Tll18GrORlwoQJjB07lgULFnDgwAHOP//8Kuf/5z//YfHixSxYsICoqChmz57N2LFjiYuLo127dgB8/vnnfPLJJ8yaNYuvv/6aiRMnMm7cOKKiovjxxx8566yzyM3NrdNzKaWUqj9twVFNLj09HYCIiIijjrm5udG+ffsTWuzO1dWVp556Ck9PT7y8vGqsk5KSwvz587nuuusAuO666/jggw8oKCgAYOXKlSQkJPD000/j6elJly5duP322+3nG2N45ZVXePbZZ+nSpQvOzs5MmzaN8PBw5s+fb693xhlncP755+Pk5MTFF19MQEAAGzdurPOzKKWUahjagtPK1aVFpakFBwcDkJycTK9evaocKy4uJiMjw16nLsLDw3F3dz9unYqxN+eddx4AU6ZM4Z577uGTTz5h6tSpJCcnExISUmWhxKioKPvrjIwMcnNzmTBhQpVuq5KSEpKSkqrEUpm3tzc5OTl1fhallFINQxMc1eS6d+9Oly5dmDt3LmeeeWaVY/PmzUNEGDNmDAA+Pj4cOnTIfry0tPSo1h0np+M3RJaXl/P222+TlZVFx44d7eVlZWW8+eabTJ06lYiICNLT0ykoKLAnOYmJifa67du3x9vbm59//pmhQ4fW67lri1MppVTD0d+4qsmJCK+++ioffvghTz75JJmZmRQUFPD5559z2223ce+999K5c2cAYmJiWLx4MXv37qWoqIgHH3zwhLcyWLBgAUlJSfzxxx9s3LjR/jF//nxWrFjBli1biI2NJTIykvvvv5/CwkL27t3Liy++WCXmW2+9lbvuuou4uDgAcnNz+emnn0hJSalTHGFhYZSVlbF3794Til8ppdSJ0wRHOcT48eNZvHgxy5YtIzo6mvbt2zNjxgxmzpzJjBkz7PWuvPJKzj//fAYPHkzXrl2JjIyscezO8cyaNYsLL7yQIUOGEBYWZv84++yzOfXUU5k1axYuLi58++23rF+/nuDgYC688EKuuuoq3Nzc7Nd57LHHuOCCC7jgggvw8/Oje/fu/O9//6O8vLxOcfTo0YN//OMfDBs2jICAAD744IMTeg6llFJ1J8ebdaKap5iYGLN27dqjyrdv307v3r0dEFHrNGvWLJ577jl27drVpPfV76NSStWdiKwzxsRUL9cWHKVsli9fzu7duzHGsHnzZp555hkmTZrk6LCUUkrVgw4yVsomMTGRSZMm2WdxXXbZZdx///2ODksppVQ9aIKjlM2kSZO0xUYppVoJ7aJSSimlVKujCY5SSimlWh1NcJRSSinV6miCo5RSSqlWRxMcpZRSSrU6muCoNsPFxYWlS5c6OgyllFJNQBMc5RCjRo3C3d0dHx8f/P39GThwIJ999pmjw1JKKdVKaIKjHObhhx8mNzeXQ4cOMXXqVCZPnkx8fLyjw1JKqUZVVFrG9LnrufmDdeQUHr158JqETApLyhrkXpuTstiSdKRBrtXSaIKjHM7FxYUbbriB0tJSNm7cCMBLL71Er1698PX1te/yXVb21w+8iPD6668zdOhQfH19iY2NZceOHfbjOTk5XHPNNQQGBhIVFcWcOXOOuu8bb7xBz5498ff3JzY2lt9++81+7NFHH+XMM8/k3nvvJTg4mKCgIJ5//nn27dvHGWecga+vL0OGDGH79u2N+JVRSrUWBcVlfLRqH1uSjvDhykTmbz7Agj8Pctn/VvDE99v4LS6dtJxC1iRkctn/VvDG0t1sSTrCG0t3k1tUyrJd6Wzan4UxhgNHCvj3xxvYtD/rqPskHspn+4FsAL7ZmMz5ry5nwqu/N/XjNgu6knFr9+N9cHBL09wr7BQY/9QJn1ZcXMwbb7wBWDtuA3Ts2JEff/yR6OhoNm7cyLhx44iOjuamm26ynzd79my++OILQkNDmTJlCv/6179YtGgRALfddhtxcXFs27YNT09Prr322ioJ0scff8zDDz/M/PnzGTJkCHPmzGHcuHFs27aNqKgoAJYtW8akSZM4ePAgCxcu5LzzzuOHH37gtddeo1u3blx77bXceuutLFy4sN5fMqVU2/DYd38yb81++/tTIvy5ZHAE//1hBzsO5vDO73ur1H9pcRwvLY4D4J3f95CRWwzAgI7+JGbmczi/hF92prHqgTNxdXZi8fY0vtqQxM/b0ygrN0yM6cQna/fTlmkLjnKYGTNmEBAQgKenJw899BBvv/02/fv3B+CSSy6hc+fOiAiDBg3iqquuYvHixVXOv/vuu4mMjMTd3Z2pU6dSscN6eXk5H330EU888QRhYWH4+/vz9NNPVzn3vffe46abbmL48OG4uLgwbdo0+vfvz9y5c+11evTowfXXX4+zszPjx48nKCiIsWPH0rt3b1xdXZk8eTJr1qxp5K+SUqolKyguY8RTS5i3Zj+9w/1wdhI6+Hvw1tUxTB3Rme1PjGPzo2fz5IX96NLem06BnvZzx/UNY0psJBm5xXi6OjO2byibko6QVWB1a+UUlvK/X/dw7su/cfOH61i6M50LBnQA4JO1+3F1FiYM6ICrs1Bebhzy/I6kLTitXT1aVJrKgw8+yEMPPcThw4eZNm0aS5YsYdq0aYDVwvL888+zZ88eSktLKS4uJjY2tsr54eHh9tfe3t7k5OQAkJ6eTlFREdHR0fbjnTt3rnLu/v37mThxYpWyrl27sn//X//xVL4+gJeXV5UyLy8v+z2VUqrCvkN5vLlsD+eeEs7CbakkZxUA8Oyl/enYzhM/D1ecnAQAZyfBz8OVKbFRTBzaCWcRLp+1grX7DnNlbCQjuwdz55ielBuDr4criZn5dA32ptxAr4d/5OXFcbg6Cw+e05uJwzrh5+HKRYMj2JCYxcShnfhqQzLfbUph0BOL2PDwGPt92wJNcJTDtWvXjrfffpuuXbvyzTffMHjwYKZMmcKXX37J+PHjcXNz46677rK30NQmODgYNzc3EhIS6Nq1KwB791Zt/u3UqdNRZXv27GHChAkN81BKqTYpr6iUS95YQUZuER+tSgQgtksgH0wbjqvz8TtNKo6/MnkQm5OO8Ldu7QFo5+1mr9MtxAcAZ4G7zu5JfFou147oTJ8OfvY6I7sHM7J7MABd2nsDcKSghP2H84kK8m6gJ23+tItKNQuBgYHccccdPPDAA2RnZ1NeXk5wcDCurq6sXLmSDz74oM7XcnJyYvLkyTzyyCOkpqaSnZ3N/fffX6XO1KlTmTVrFqtXr6a0tJTZs2ezceNG3U1cKXVSfo/PICO3iM7tvXn8gr788O+RfHxDbK3JTWXh/p6M7RuGyPFbW246vSvPXjagSnJT3Zg+obxx5WAALnljBVPfW836xMOUtYEuK23BUc3GrbfeygsvvMCaNWt47LHHuOCCCyguLmb06NFMmjTJPsOqLl566SWmT59Or1698PPz4/HHH+frr7+2H588eTKZmZlMmTKF1NRUevbsyQ8//FClW0sppU7EnylH+GhVIm7OTiy4bSTuLs6ODgkRYXSvEHqG+rIzNYelO9NZujMdAH9PV778x2l0DfZxcJSNQ4xp/VlcaxMTE2Nq6q7Zvn07vXv3dkBEqiHp91GplmfmTzt59RdrHa9/ndGNO8/u6eCIjlZWbli37zC/7Exjx4FsftmZzoyL+nHl8ChHh3ZSRGSdMSamerm24CillFJ1kJlXzANfbiE8wIOrYqPoUqnl47vNKbT3cWfuDcPpEerrwCiPzdlJGNY5kGGdAykqLaPnQwvItE0/b400wVFKKaWOo6i0jC/XJ7NkRxqLtqUCsO9QPi9dMZCUrEK+WJ/EvkP5PHZ+32ab3FTn7uKMj7sLh/Lqn+B8vzmFvh38cXNxwsfdBX9P1waM8ORpgqOUUkrVoLi0nDl/JJCaXcjblRbiu2JoJ+at2U/sfxeTV2wtIDokqh0Th3ZyVKj1EuTjRma1BMcYQ1m5oaCkDF+PYycsKVkF/HPuBvt7V2fhzatiGN0rpNHiPVGa4CillFLVpGUXcs8Xm+0DcgFEILZzEPef05uN+7PYcTAHdxcnPrv5VE6J8K911lNzE+jtxtKdafwel4GLs/DWsj0s3pFmP77nv+ccc92cirV9+nf0Z9KwSJ5buJNrZ6/hvWuHMrpn80hyNMFpZYwxLe6HTP1FB/0r1Tw8/v02e3IzJTaS4Z2DOK9/uP3364Lb/s6RghJyCkvo2M7LkaHW28juwWxIzGLKO6tqPB6fnnvMLrcUW4Lz/OUD6BbiS+f23lzx5kpe/DmOUT2Cm8XfIU1wWhFXV1cKCgrw8mqZP2wKCgoKcHVtXv3YSrU1ZeWG3+IyAHj6klOYODSyxnr+nq7NbtzJifjn6G50D/HB3cUJVxcn+nbww83ZiXeXJ/Dy4jhm/rSTG//ehV2pucxdvY/SMsPU06KZtWwPezPyAGvNHoDYLkFMH92V137ZzX+++ZMnLuznyEcDNMFpVUJCQkhOTiYiIgJPT89mkUGrujHGUFBQQHJyMqGhoY4OR6k2q6SsnIe+2sqRghJenjSI8217O7VGbi5OTKjh+e4Y04OikjJmLdvDQtugajdnJ4rLyrnvyy14uTlzVu9QBkcF4O3+Vxpx21k9SDiUz6dr93P/Ob3wcnMhp7CE+7/cQnZhKdeNiGZUE3ZfaYLTivj5WatZpqSkUFJS4uBo1IlydXUlNDTU/n1USjW9X3em23fhHmnbKqEtuv+c3pzXvwM7DmYT3d6b/h39+WVHGs5OTgzo6E+In8dR57g6O3HxoAjmbz7AnynZDI0OZP7mA3y/+QAAR/KLOb0Ju680wWll/Pz89A+kUkrV07YD2QCseuDMKntAtUWndPTnlI7+9vfj+oUfp7alYm2gfYfy6Rnmy31fbgGsxQ9fWRLPl+uTuWRIx8YJuBrdi0oppZQClu5M4/lFu4gO8iK0hhYKVbuIAE+cnYS3f9vDgMcWAjD1tGhuP6sH3UJ8eHf53iabTKEtOEoppdq8krJypr63BoBOgTpRo77cXJwY0zuUBX8epHuID8O7BPLweX1wchKevLAffh6u2kWllFJKnazycsMna/eTX1yGt5sz5w/sgJeb9acvK7+Yf3y0ngGdAliXcBiwWiD+c14fR4bc4r0xZTCH8ooJ8narkszEdglq0jg0wVFKKdVq/bH7EPfbxoEAvP37Xl6ZNIje4X78sjONP3Yf4o/dh+ge4sOVwyN56Nw+eLo5fhfwlkxEaO/j7ugwNMFRSinVOt3/5WY+Xr0fZyfh93tHM/2j9axPzOLuzzfxwPje3P7JJgCeubQ/lw7ueMxVe1XLpIOMG5iIdBKRX0Rku4j8KSK32soDRWSRiMTZPrezlYuIvCwi8SKyWUQGO/YJlFKq5SkvN8Sl5lBUau0NtSYhk49XW9O9p4/uRri/J7OuisHT1Zk/U7KZ/La1em90kBeXx3TS5KYV0gSn4ZUCdxpjegOxwHQR6QPcByw2xnQHFtveA4wHuts+bgTeaPqQlVKqZftmUzJjXljGaf+3hKTD+Vzx5koAfrtnNHeM6QFAsK87c28YTsUknu4hPjx72QBHhawamXZRNTBjzAHggO11johsByKAC4BRtmpzgKXAvbby9401b26liASISLjtOkqpNiwuNYfswhL6dvDHw7XljAsxxlBuwNnWKpJbVGptB+DcOP9Tb00+wguL4gA4lFfM357+BYAXJw48akbUoMh2fDN9BBm5RZzZW1cNb820BacRiUg0MAhYBYRWJC22zxXrVUcA+yudlmQrq36tG0VkrYisTU9Pr35YKdUK3fzhOi55YwUDHltIZl6xo8Ops8e/38bgJxaxJz2X+LRc+j3yU5WBvg3tmZ92kpiZzyMT+hDiaw1u7dzem3NOqXlhugGdAjS5aQO0BaeRiIgP8AVwmzEm+zjz/ms6cNQqSMaYN4E3AWJiYnTLaaVaqZKycub8kcDP21PZnW5taFhUWs7gJxax+sEzCfFt/gvQvbc8AYAznvvVXvb5uiSuOTW6ysq4DaGotIyVew5x7Yhorh3RmSuHR+Ek4NJIrUWq5dAEpxGIiCtWcvORMeZLW3FqRdeTiIQDabbyJKBTpdM7AilNF61SyhFKy8oREXs3DkBhSRlnPf8rSYcL7GUf3xDLb3HpvL50N+v3Ha7TcvmOdCS/6j543UJ8OL1HMN9sTOamD9ay/L4z6rTQ229x6byyJJ6YqHbEpeUSHeSFiHDvuF5VvmbLdmVQXFpOTFQgYC00pxRogtPgxPrJfQfYbox5vtKhb4FrgKdsn7+pVP5PEZkHDAeO6PgbpVq3RdtSufPTjYzo1p7XrxzMfV9sIT49l5v+3oWkwwX07eDH7GuH0c7LFRdnJ/p39Of1pbvtLTrN2fr91oJ5V58aRfdQX6YMj0RE6BHqw71fbGFXai49w3zt9cvKDc8v2snBI0VEBnpRZgyHcov4aFUiAKv3Zla5/uDIdnQI8OD6OWvp08GPpTutLvv+DdwypFo+TXAa3gjgKmCLiGy0lT2Aldh8KiLTgETgMtuxH4BzgHggH7i2acNVSjW25KwCOvh7ICIkZxVw0wdrKTfw49aD/HveRr7bZDXafrf5AB6uTnw9fUSVAbne7i6E+3uwOy3XUY9QZ+v3HcbZyWpp8Xb/60/M8M7WKrZjX1zGF7ecypCoQDbtz+KC15Yf81qPX9CX9fsOk5iZT5dgHz5fl8TNH66zH0+zJTdn9grR7RXUUTTBaWDGmN+peVwNwJk11DfA9EYNSinlMM/+tIPXftnNrKuGMLZvGM8u2EG5gScv7MdDX2+1JzcA321KYWh0uxpnG3UN9mF3RvNvwVmbcJje4b5VkhuAqKC/EpDLZ61kfL8wfD1c7cf2HcoHYMujZ5NdWIq/pys+7i5cfWq0/byDRwr5PT4DgDN6hdAzzJeVew7x/MSBjfxUqiXSBEcppRpJTmEJr/2yG4B9h/LIyC3ihy0HmXpaNFNio+gR6ss1767m5tO78sLPuwC4aFDHGq/VIcCDT9cmsWDrgWY9Dmdnag5j+x49Q0lEmHPdMJ78fhtxabl8v9nqiR/VM5jZ1w7j7d/20LGdJ74ervbEp7o51w1j2a50eoX7EurroYvzqePSBEcppRrJtpRs++us/BJW7D5EcVk5Fw+2VoIY1jmQTY+cjZuLExcO6kCYvwfuLjWvdzP+lHA+XZvEiz/HMbZvWJPtyHwicgpLyMwrJjLQu8bjp/cIJm5oJ56cv91eNqpHMADXj+xS6/WdnYTRvUJqracUaIKjlFKNZsP+LPvrtfsO27ttugb72MsrZv1EBdWcFFQY3TPE3q31zu9765QQ1FdGbtFRO0HXxf5Ma/ZX5HHGw0wc2on4tFz6dPAj6XABVwyLPKlYlToWTXCUUqoBZeQWsWL3Ic49JZwl29Po28GPsnLD6r2ZrN6biZuL01HjU+rqiqGdWLw9lSfnb8fTzZkrh0c1cPSQkJHHqJlLCfVz5/8uPoUzetV9Qbz9h61xNMdLcHw9XHnqkv4nHadStdEFA5RSqgFd+94a/vXxBpbFpbM15QhDowN57Py+9uNn96n/Crouzk68eMUgAB78aitfrJo3GGUAACAASURBVEvicF4xm/ZnkV9cetKx/7ornVEzlwKQml3E67bxQ3W1P7P2BEeppqItOEop1UAKisvYknwEgKnvrQGge6gPw7sE8f51w+gS7E3Hdif3x9/f05W7x/bk2Z92cudnm+zlwb7udGrnSWJmPv6eruxOz+PfZ3Tjb92D6RXui98xBu5WMMbw9m97AJg8PBI/D1fe+X0PRwpK8Pe0zi0vN+w4mEOPUJ+jVgouLStn5Z5D+Hm44O91/Hsp1RS0BUcppRrI3NXW4nTdQqwxNiIwNNpaYffvPYJPOrmp8I9RXVn/8Bj7+5Hd2xMZ6MX6xCwycovtCwK+vCSey2etYOwLy3hh0S7Offk3Yp5cxOq9mWTmFfPIN1vJLbJafuLScvktLoObT+/Kfy86hfP6h1NSZqpMY3/rtz2c8/JvfGx7zgrl5YZrZ6/h5+1pDOgU0CDPqNTJ0hYcpZRqAMYY3lq2h1O7BPH+tGHW4nTtvRtltpOIEOjtxtbHxrJ+32FiuwTh5uLE8vgMAr3d2JaSXaV158CRQl5aHGd/f/msFQyKDGBDYhYd23lx9WlR9nVoxvULA6BvBz98PVzYlZqDMYY/U7L50zYr7OFv/uTAkUJeX7qbIVHt6Bnmy29xGfh5uPDqpMEN/rxK1YdY68ypliQmJsasXbvW0WEo1aptTT5CcVk5gyPb1VrXGMO+Q/mMmrmUx87vyzWnRTd+gLXILy7lz5Rs0nOKePjrrWQVlPDFLafx6850+5o7Ffw9XZkwIJwPVyay7qGzCPKxduQ+75XfCPR259QuQTy9YEet91z9wJmE+DX/zUBV6yIi64wxMdXLtQVHKaWqKSot47xXfgfgm+kj6BzsTXFpOe1tf/grKys3XPHmCtYkWHsw9Qj1PaqOI3i5udi7x87uE0q5saakD+wUQE5hCS7OTvy9e3tmLtzJ+sQsPlyZiI+7C4HebvZrRAV5M3/zAZbtSreXje4ZzJCodsxcaCVJsV0Cuen0rrT3dtfkRjUrmuAopVQ1y3Zl2F9X7JXk7uLE6gfOOmoA7ZIdaaxJOMxZvUNwdhIGNsMxKNUHBD90Xh/76y+7tWfnwRwSDuXh7+lapUutT7gf820rDj918Sn07xhAqJ87/p6unHNKuH3tHmddUVg1Q5rgKKVUJUfyS3h6wQ7aebny6U2nsnJvJq8sjiMtp4i4tBxibK0iFWb+tJNOgZ68OnkwHq41r0Lc3PUM862yw3eFS4d0ZNG2VJ67fECVxQkBulR7r1Rzo7OolFKqkoe/2Up8Wi73jOtF91BfroqN4vObTwPg3x9vYGvyESrGLu47lMfO1BwmxnRqscnN8YT6efD19BFHJTdKtQSa4Cil2rTCkjJufH8ti7enMvKZJXy7KYV/n9mdSZW2EIho50n3EB9SjhRy3iu/8/rS3azbd5ir310NwLDOQY4KXyl1DNpFpZRq077ffICF21JZuC0VsNaY+efoblXqODsJi+44nZSsAsY8/yvP/rTTfuyGkZ0ZGl37TCulVNPSFhylVJs2r9KidaN7BnPPuF72DTCr6xDgyYQBHQAY0CmAeTfG8sA5vZvlzt5KtXXagqOUarN2peawdt9h2nm5EhXkzWtX1r5I3fTR3RjQKYALB0bg6db6xt0o1VpogqOUarPmrkrEzdmJxXeOqrL+y/F0CvSqMj5HKdU8aReVUqpNKigu44v1SYw/JazOyY1SquXQBEcp1eakZRdy7iu/kVNYymRtjVGqVdIuKqVUmzPsv4sB6NjOk2GdA2uprZRqibQFRynVpuxKzbG//viGWJ0BpVQrpQmOUqrNKCwpY/Jbq3BzduLXu0fRKdDL0SEppRqJdlEppVq97MIStiQd4f0VCWTkFvHEBX3tG0UqpVonTXCUUq3ePZ9tZsGfBwH45+huTImNcnBESqnGpl1USqlWLa+o1J7cnBLhz51n99BxN0q1AdqCo5Rq1eZvPgBYA4oHRQZocqNUG6EJjlKqVZu7OpFuIT7EdgnU5EapNkS7qJRSrdb2A9ls3J/FpGGRmtwo1cZogqOUarWenL8NNxcnLh4U4ehQlFJNTBMcpVSrlHgon+Xxh/jn6G60072mlGpzNMFRSrU6RwpKuOnDdQCc1z/cwdEopRxBExylVKvz/eYUth/IZsZF/egS7OPocJRSDqAJjlKq1fl6QzJdgr11p3Cl2jBNcJRSLV5JWTkpWQWAtZnmmoTDXDG0k86cUqoN0wRHKdWilZaVM3HWCk57agnxabl8vDoRN2cnLhnc0dGhKaUcSBMcpVSL9p9v/2R9YhZgdU3N+SOBsf3CCPJxd3BkSilH0pWMlVItVlZ+MXNXJdrfv/pLPADXnKqbaSrV1mkLjlKqxfp2UwoA3//rb1xtS2puOr0LMdGBjgxLKdUMaAuOUqrFMcZw4Eghc1cl0jXYm34R/vQK82Vc3zAGR7VzdHhKqWZAExylVIsz5oVlxKflAjDzsgEAuDg7cVq39o4MSynVjGgXlVKqRSksKbMnNwAXDuzgwGiUUs2VtuAopVqUnQdzAJhxUT/O7hOGi7P+n6aUOpr+ZlBKtSi/7kpHBMb0CSXYV6eCK6VqpgmOUqpFWbX3EH3C/Qjx9XB0KEqpZkwTHKVUi7LzYC59wv0cHYZSqpnTBEcp1WLEp+WSkVtEzzBfR4eilGrmNMFRSrUYH67ch7uLExcMjHB0KEqpZk5nUSmlmr0Vuw/xyZpEvt6Ywhm9QnRwsVKqVprgKKWatYzcIia9tdL+/rIhuku4Uqp22kXVwETkXRFJE5GtlcoCRWSRiMTZPrezlYuIvCwi8SKyWUQGOy5ypZqXlKwCsvKL+XHrQXvZg+f0Zly/MAdGpZRqKVp9C46IjADeAHoBfxhjRjXyLWcDrwLvVyq7D1hsjHlKRO6zvb8XGA90t30Mt8U5vJHjU6rZM8Zw2lNL7O89XJ1448ohjO4V4sColFItyQm14IhIsIi8LiIJIlIkIqkislhExjRWgA3gJWAT0BW4uLFvZoxZBmRWK74AmGN7PQe4sFL5+8ayEggQkfDGjlGp5i7hUH6V929fPVSTG6XUCTnRFpwvAC9gGhAPhACnA0ENHFdD6ga8ZozZ78AYQo0xBwCMMQdEpOI3dQRQOa4kW9mBJo5PqWblqw3J9tf/HN2Nv3XXTTSVUiemzi04IhIAjATuM8YsNsbsM8asMcbMNMbMq1QvQUTuqnbuUhF5tVqd/4jIbBHJEZH9IjJRRAJEZJ6I5NrGq5xdS0zuIvKirSWpUERWisjfbMeiRcQA/sC7ImJEZGpdn7eJSA1lpsaKIjeKyFoRWZuent7IYSnlWKv3HmJQZAB/PjaWO8/u4ehwlFIt0Il0UeXaPs4XkYZYI/02YDUwGPgUq+tmLvADMBBYBnxYy72eASYC1wGDgC3AAls3z34gHMi33Ssc+KQB4q6P1IquJ9vnNFt5EtCpUr2OQEpNFzDGvGmMiTHGxAQHBzdqsEo52t6MPLoF++Dt7oJITf8HKKXU8dU5wTHGlAJTgSlAloisEJGZIlLfQbE/GWNeN8bEAY8A7kC8MeZ9Y0w88AQQDPSr6WQR8QZuAe41xsw3xmwHbgZSgenGmDJjzEGsFpEjxpiDxpiCesZ6sr4FrrG9vgb4plL51bbZVLFYcWr3lGrTcgpLSM0uIrq9t6NDUUq1YCc0yNgY8wXQAZgA/AicBqwUkQfqce/Nla6bi9XSsqXS8VTb52ONLOwKuALLK12nDFgB9KlHPA1CRD62xdBTRJJEZBrwFDBGROKAMbb3YLVW7cEaz/QW8A8HhKxUs7JxfxYA/SL8HRyJUqolO+Fp4saYQmCR7eNxEXkbeFREZhpjioFyjh5b4lrDpUqqX7paWcVYlGMlYVKtXvVrOYQxZtIxDp1ZQ10DTG/ciJRqWf7YfQhnJ2FwZICjQ1FKtWANsdDfNqxEqWKsTDrWeBcAbGNoejXAfaqLB4qBv1W6lzNwqi0mpVQLk5pdyKdr9nNa1yB8PWr6v0gppeqmzi04IhIEfAa8i9W9lAPEAPdgLWKXbau6BLhORL7FSnYepOYWnJNijMkTkTeAp0QkA9gL3A6EAq839P2UUo1v1q97yCoo4c6zezo6FKVUC3ciXVS5wErgVqy1ZdyBZKyZT09Wqvd/QDTWQNpcYAbWuJ3GcK/t83tAALABGKcDdZVqedJyCvls7X7O6x/OwE7aPaWUOjliDQNRLUlMTIxZu3ato8NQqkHd98VmvlyfzLf/GkGvMD9Hh6OUaiFEZJ0xJqZ6uW62qZRyuLJyw6JtqYztF6bJjVKqQWiCo5RyuN/jMziUV8yYPqGODkUp1UpogqOUcrjnF+4kyNuN0T11lW6lVMPQBEcp5VAlZeVsO5DNpTEddWq4UqrBtOoER0TuEpEER8ehlDq2uNRcSsoMfcJ17I1SquG06gRHKdX8/bE7A4Ch0YEOjkQp1ZqcVIIjIm4NFcjJEBFt11aqBSoqLWPOigR6hfnSIcDT0eEopVqRE0pwRGSpiLxh20U8HVguIv4i8qaIpIlIjoj8KiIxlc45KCITK71fbqvnYnvfXUSMiETY3k8RkTW2Omki8lnFMdvxUbb654jIahEpBsbajt1ju1+uiLwP+JzUV0cp1aheWxLP/swCrh/ZxdGhKKVamfq04EzB2uhyJHA1MB+IAM4DBgHLgCUiUrEf1a/AaAAR8cLa3qHI9hlgFBBvjEm2vXcDHgEG2K7ZHvi4hjieBh7C2udqlYhcjrWi8iPAYGAncEc9nk8p1UR+3HqQYdGBXDqko6NDUUq1MvVJcPYaY+40xuzA2lRzIHCpMWa1MSbeGPMwsAe4ylZ/KbYEBxhhOza/UtkoWx0AjDHvGmN+MMbsMcasBm4BRopI9d+AjxpjFtrqpQO3AXOMMbOMMbuMMTOA1fV4PqVUE8jKLyYuLZfTdWq4UqoR1CfBWVfp9RDAC0i3dQvlikgu0A/oaquzFOghIh2wkplfbGWjbMdPp1KCIyKDReQbEdknIjlAxZ4EkdXiqL5XQW9gRbWy6u+VUs3ELzvTAIiJaufgSJRSrdGJbLZZIa/SaycgFau7qrpsAGPMdhFJxUpoRgEvAmuAV0SkD1b31lIAEfEGfgJ+xmoBSsPqovoNq+vqWHEopVqYrzak0Lm9t86eUko1ivokOJWtB0KBcmPMnuPU+xU4F2vcza/GmDQRyQDuoer4m15YCc0Dxpi9ACJycR1j2Q7EAu9WKout85MopZqMMYbNSVmM6xuGk5M4OhylVCt0suvg/AwsB74RkfEi0llEThWRx0SkcqvOUmAiEGeMSbOV/Yo1YHlppXqJWAOQ/ykiXUTkXOCJOsbyEnCNiNxgm5l1PzC83k+mlGo0cWm5ZOWXMKBTgKNDUUq1UieV4BhjDHAOsAR4C2vm0qdATyClUtVfAGeqJjNHldkGC18DXAhsw5oRVaeZUMaYT4BHgRnABuAU4PkTfSalVOP7cctBRODMXiGODkUp1UqJlaOoliQmJsasXVt9jLVSzV9JWTn5xWVMnLUCXw8XPrv5NEeHpJRq4URknTEmpnr5yY7BUUqpOnv8u218sHIfAP85r4+Do1FKtWa6F5VSqkmUlpXbkxsvN2fO7R9eyxlKKVV/2oKjlGoS328+AMDdY3sytm8YoX4eDo5IKdWaaYKjlGoSn63bT6dAT/4xqisiOjVcKdW4tItKKdXotiYfYXn8Ia4YGqnJjVKqSTRogiMi34vI7Aa4jhGRSxsgJKVUMzBvTSLebs5cdWqUo0NRSrURzbWLKhw47OgglFINIzGzgK4hPvh5uDo6FKVUG9GsuqhExA3AGHPQGFPk6HiUUg0jLbuQEF8dVKyUajr1TnBExEtEZtt2EE8VkQeqHU8QkbuqlS0VkVer1XlURN4VkSzgI1u5vYtKRKJt7y8RkUUiki8i20RkTLVrnysiO0WkUESWicgVtvOi6/uMSqmGkZZTRIifu6PDUEq1ISfTgjMTGANcApwJDAL+Xo/r3AHswNqI84Hj1JsBvAwMwNqNfJ6I+ACISCTwJTDfdvxl4Jl6xKKUwxljMMawLSWb5KwCDh4pJCEjz9Fh1VteUSmZecWEaguOUqoJ1WsMji2xmAZcZ4z5yVZ2LZBUj8v9aoypSzLygjHmO9u9HgCuBgYCvwO3AHuAO237Y+0UkR5YSZFSzV5xaTlLdqSyKekIc1cl4uwkZOYV24+H+rmz6oGzHBjhiSssKcPN2Ykftx4EILZLoIMjUkq1JfUdZNwVcANWVBQYY3JFZEs9rlXXTZU2V3pdsZFnxU59vYA1purGWqvqEYtSDnH7JxuZv8VaCK9HqA/+nq5cP7IzTiI89eMOUrNbzpC09JwiXl0Sx7ebUvBycyE9t4g+4X4MjdYERynVdOqb4NRlIYvyGurVNIWirm3vJRUvjDHGtpZGRRebALprqGqRDuUW8cPWA4zpE8pzlw84aqZRZl4xc/5IcExw9fDaL/HMWbGPfhF+OIvg4iw8d/kAnJx0/RulVNOpb4ITj5VwxGJ1DSEi3kA/YLetTjrWdG9sxz2wWlo21DfY49gOXFCtbFgj3EepBhGXmkNkkBdLtqfxw9aDGAPT/ta5xmnU3m4uFJWWU1pWjotzs5r4WKOEQ3n07eDH9/8a6ehQlFJtWL0SHFt31DvA0yKSjtVl9B/AuVK1JcB1IvItVrLzIDW34DSE/wF3iMhM4C2gL3BTRbiNdE+l6uWln+N44eddR5X3i/Cvsb6Ph/VjmldUhr9X809wkg8X0Lm9t6PDUEq1cSfz2/Iu4BfgK9vnrcCySsf/DyvJ+QZYiDUYeP1J3O+YjDH7sGZznQ9sAm4HHrMdLmyMeypVH+k5Rby0eBd9wv24ZVRXBkcGAHDbWd3xca/5/w0fd+v/htzi0iaLs75Ky8pJOlxAx3Zejg5FKdXG1XslY2NMHtZMpquPcTwbmFSt+PVqdaKPca5Uep1ADWN+Ktexvf8e+L7ivYjcCmRjtR4p5VDzNx8gKsiLP3ZnUG7g+YkD6BXmV6dzvd0rWnCad4KTnlPEMwt2UFBSxrDO7RwdjlKqjWuuWzWcMBGZjrU+TjrW2KCHgdnGmHKHBqbavISMPKbPtRov3V2cGNm9fZ2TG/grwTn7hWU8c2l/Lo/p1ChxnqznF+3ks3VJnNkrhFE9Q2o/QSmlGlHz79Cvu25Y3WXbgSewxuXc7dCIVJu371Aek99aaX9/atcgnrt8wAldI9z/rwXyZv1qjeEvKzcUlpQdVbe83PDp2v3sSs2htKyc+LRcnvh+GzPmb+NIQclR9RtSZl4x4f4evDN1KB6uzrWfoJRSjajVtOAYY27HGnujlEMVlZbh4uSEs5PwzIKdHM4v4bXJgzmrTwjuLif+h79XmB9rHjyLD1bu45UlcfyyI427P99EZl4xXYN9ePGKgfTtYA1Q/nZTCvd8bi0Z5efhQnbhX91a8zcf4L8Xn9JorSv5xWWE+etqxUqp5qE1teAo1Wh2Hszh+80ptdbbcTCb0c8uZcwLv3Lx68uZv+UAk4dHcm7/8HolNxWCfd3p1M4TY+Da2Ws4UlDCuf07EJeWywuL4uz11iRkAuDqLFWSmwfO6UXKkUKun7OWrPzio67fEPKKSo85UFoppZpak/w2sm14uRcYaoyp68rFJ3qPS4HPqg8+VupkFJWW8fWGZO79wlqke3y/cJyrLVhnjGHlnkxyCku4+/PNuLs4kVNYSlFJOY+d35crhjXMmJlQv79aR96dOpSR3YPxcXdhwdYD9vIDRwrpE+7HJzfF8uBXW7nmtGi6Bfvg7+XKKREBTHprJZ+u3c+Nf+/aIDFVlldURrCvbqiplGoemurfrf1Yi/5lNNH9lDpppWXlXPX2albbWkXAWnU4xK9qN8zHq/fzwFdWAuTsJHx0/Qj6dvDDttp2g4mJbsdFgyK4Y0wPOgVa07Cjgrw4nF9CdmEJRSXlLNmRxpm9QvD1cOXlSYOqnB/bJZCh0e2YvTyBG//eFWMMBSVleLk1zK+BvOJSvBvoWkopdbKa5LeRMaYMONgU91Kqoby0OI7VCZncdXYPuof6ctMH6zhwpJC84jKe/WkHyYcLCPZ15+ftaQR5u3FlbBQXD4ogupEWufNyc+GFiQOrlEXaEp3+jy60l1UkP9WJCGP7hvHk/O2kZheyam8m//54A5OGRfLweb1POtHJKyq1z/hSSilHq9NvI7H+Fb0ba3XgDlhbNTxtjPmwUvfTlcA/gBggAfi3MWah7fyKOkONMWtFxBV4DrgUCALSgI+MMffZ6rcDXsRauM8DWA7caoz5s1JMV2PNlgrGWlDwxxringA8irWy8QFgLvCYMaZxBiGoVmPdvkxeWRLPZUM68s8zurM1+QgA//p4AwezCykutVYf8HR15s4xPbhlVFeHbKMwKDIAV2ehpMxasHvy8Ej+dUa3Y9Yf2T0Y2M7w/y62l328OpGikjIePLc3RaXltPdxx83l6GcpLSvnz5RsMvOLcXN2ol8Hf1ychYzcIvKKyjicX4KXu86eUko1D3X9d+tJrGRkOrATOBV4S0QOAxVJxzPAHVi7fk8HvhGRbsaY5Bqu92/gIuAKrGSoI9Cz0vHZtvcXAIeBGcACEelhjCkQkeG2Og8DnwGjgf9WvoGIjAU+Am7FWmE5EmvquDvWKsxKHdN3mw7g7uLEI+f3BaBbiA8jugWxMTGLsnLDExf2Y8rwyAbvhjpR4f6e7HpyPK8v3U23EB/O7hN63Jh6hvny9x7BLNtlrX957YhoikrLmbsqkS83WD+qI7u354Npw6ucN291IjPmbyenlsUGtYtKKdVciDHH36rJtolmBnC2Mea3SuUvAj2wWm32Ag8ZY2bYjjkBO4BPjTEP1dCC8zJWq8pZploAItId2AWcboxZZivzBxKBO40xb4vIXCDYGDOm0nlvA9MqBhmLyDJgkTHmiUp1LgQ+BHyr37cliYmJMWvXNspYbYU1aPhvT/9C73Bf3r5maJVjRaVlHMkvOWocTkuSmVfMmoRMfD1cGBodSNLhAh76egsbE7PIK7bW1lly5+l0CfaxnzN65lLcXZyYProboX4ebNx/mKU70xkUGUCwjzvtfd1ZszeTKbFRdA/1ddSjKaXaIBFZZ4yJqV5el3+3+mB1Ey0QkcpJgStW60uFFRUvjDHlIrLKdm5NZgOLgF0ishD4AfjRtupwb6C82vWOiMiWStfrDXxX7ZorgGmV3g8BhonIvZXKnABPIAyry0qpo2xNziY5q4Bbz+p+1DF3F2dC/Fp2N0ygtxtj+4bZ33du781H18dSVm74akMyd3++iQ9W7uORCVbrVdLhfPZm5PHgOb2ZMKADAMM6Bx41E+u8/h2a7iGUUqoWdUlwKjrjJ2C1olRWQg37RNXGGLPe1qozDjgDmANsEpExtVyvIsGqyz2dsDbc/KyGY7o/lTpKXlEp6TlFvPZLPC5Owlm9Qx0dUpNydhIuHdKRbzYm897yBPZnFpBbVEJCRj4A4/qF1XIFpZRqPuqS4GwDioAoY8yS6gdtiQpY+z8tsZUJMAz4/FgXNcbkYCUfn4nIbGAl1nYL27CSk1Ox7U4uIn7AKcB7lWKKrXbJ6u/XA72MMfG1P6JSMG3OGlbusaaE33V2DwK93RwckWM8dUl/Xv8lno9W/fX/zBMX9jvm7CyllGqOak1wjDE5IjITmGlLXJYBPlgJRTlQMT/1FhHZBWzBGpcTBbxR0zVF5A6sLqKNWK1Ak7F2/k4yxuSLyDfALBG5EcjCGmScjTULCuBl4A8RuR8riRqFNWi5sseB70VkH/ApUAr0A4YZY+6p7blV25JdWGJPbk7rGsQNf+/i4IgcJyLAkxkXncIto7qSdLiA3mF++Hu5OjospZQ6IXWd1/ow1nTru7BmTS0CLsEaOFzhPqxZVJuwup4uMsYkHeN6OVjTzldjtbQMBMYbY/Jtx6+1HfvW9tkLGGeMKQAwxqzEGm9zC9asrYtt8dkZY34CzsWaYbXa9nEfR3ezOZyIjBORnSISLyL3OTqetqa0rJwr31oFwHOXDeDDacNPaluF1qJjOy9iuwRpcqOUapFqnUVV6wWaYBuG1kxEnLFmjY0BkoA1wCRjzLZjnaOzqBrGvNWJvPnbHvak5wEwJTaSx8/vh5OT7vahlFItxcnMolKNaxgQb4zZAyAi87DW/zlmgqNO3sb9Wdz3pbW9Qq8wXyYM6MD00cdeIE8ppVTLogmO40Vg7dVVIQkYXr2SbTzSjQCRkZFNE1krtSXpCBe+thyABbeNpFeYn4MjUkop1dBOem15Y0yCMUa0e6reauoPOarf0BjzpjEmxhgTExwc3ARhtV6vLInDy82ZWVcN0eRGKaVaqabfPEdVlwR0qvS+I5DioFhavfeW72XhtlSuG9G5ymJ3SimlWhdNcBxvDdBdRDqLiBvW/lz/396dx8dV1nsc//yStElbui9032gpbYECLauAlEVAdgRFQBDxohdQrysiXsXLIqggqMgisnnRynopa23ZVGxLKVKgCyVt6b43bUnTJpmZ3/3jOelM0rRkmWQmM9/36zWvmTnnzMwvx4N8eZ7nPM/kDNeUk6piCW55cQFH7dNT421ERHKcxuBkmLvHzOxqYApQCDyQumq6pIe7c8/ri6iMJfjiYYPp0F63gYuI5DIFnCzg7i8Q1uOSNEoknLKKKnruVczspWXcPnUhAOMGdstwZSIi0tLURSU56+cvzmf8jdNYsGYr763cAsDvLjyYwT215ICISK5TC47kpO8/PofHZ4eJtH//6iJmLtlIn87FnHZAvwxXJiIirUEtOJJz3H1nuOlSUsTkOavYVhnnrosOISynJiIiuU4BR3LOomjphRvO3p8fnz4GgBvP3p9Dh/bIZFkiItKK1EUlOee5d8M0QieO7kO/rh04ekQv+nfrkOGqmnM4sQAAIABJREFURESkNSngSM7YUF7JxvIqHpm+lGNG9qJf1xBqFG5ERPKPAo7khE3bqphw4zQgjLv58WljMlyRiIhkksbgSE54/t3k6hZ3XXQIo/p2zmA1IiKSaQo4khNemruGogLjwcsO5ZiRWoxURCTfKeBIm1e2rYoZizdxxbHDmTiqT6bLERGRLKCAI23e1PlriSecU/bX6uAiIhIo4Eib99y7qxnUowMHDOia6VJERCRLKOBIm1a2rYo3Sjdw2gH9NUuxiIjspIAjbdrkOauIJ5zTD9QaUyIikqSAI23Wqs3b+dXfPuDQod0Z279LpssREZEsooAjbdbPX1xAIuHcdv5B6p4SEZFaFHCkzXpneRnHj96bwT07ZroUERHJMgo40iaVV8ZYUbadEb33ynQpIiKShRRwpM1JJJzz75mOOxw6tHumyxERkSykgCNtzrPvrmL+6q3ccNZYjhrRK9PliIhIFlLAkTbF3bnlxQX06VzMOYcMzHQ5IiKSpRRwpE2Zu2orq7fs4Aen7MdexUWZLkdERLKUAo60KVPnraXAYOIorRguIiK7p4Ajbcrby8oY3a8LPfcqznQpIiKSxRRwpE2Zv3oro/tp1mIREdkzBRxpM1aUVbChvIr9tSyDiIh8AgUcaTMmz1kFwOHDe2a4EhERyXYKONImVMUS3P3aIg4b1oNRe3fOdDkiIpLlFHCkTZi+eCMf74jxtWOHU1CghTVFRGTPFHCkTZgydw0d2xfyKc1cLCIiDaCAI1kvFk8wdd5ajhvVm5J2hZkuR0RE2gAFHMl6f/znEtZ/XMk5B2tpBhERaRgFHMlqG8or+e0rpZywXx9OGrN3pssREZE2QgFHslZVLMF3H5vD9uo41352dKbLERGRNkQBR7LWY28t5/WF6/n2iSMZ0WevTJcjIiJtiAKOZKXNFVX87pVSxg/pzlUTR2S6HBERaWMUcCQr3frSAjaUV/KT08dgpnlvRESkcRRwJOvMWLyRSbOWc8mRQxk3qFumyxERkTZIAUeyirtzw3PzGNS9I/910shMlyMiIm2UAo5klafeXsncVVu5euIIupS0y3Q5IiLSRingSNbYtK2KG5+fx/gh3TlvvCb1ExGRplPAkayQSDjff3wO5ZUxfn7uAVpQU0REmkUBR7LCmx9t4uUF6/j2Sfuy796dM12OiIi0cQo4khWenbOKDu0K+fJRQzNdioiI5AAFHMm46niCF99fw4lj9qZj+6JMlyMiIjlAAUcy7uF/fcSmbVWccWC/TJciIiI5QgFHMqoqluCuV0s5dGh3jt+vT6bLERGRHKGAIxn155lLKauo5hvHj6SoUJejiIikh/6NIhmzuaKKX0/7kKNH9OKYkb0yXY6IiOQQBRxpVYvWlxOLJwB4ZPpStmyv5rrTRmtBTRERSSsFnDQys/PNbK6ZJcxsQp1915pZqZl9YGYnp2w/JdpWamY/bP2qW897K7Zwwm2v87tXS9lSUc0j0z/iUyN6Mrpfl0yXJiIiOUb35KbX+8C5wL2pG81sDHABMBboD0wzs32j3XcBJwErgFlmNtnd57Veya0jFk/w42feB8J6U+s/rmRDeRUPnjI6w5WJiEguUsBJI3efD9TX3XIWMMndK4ElZlYKHBbtK3X3xdHnJkXH5kzAWbl5O6/MX0tBgTFn+WbGD+nO7KVlPDpzGeccPIADBnbNdIkiIpKDFHBaxwBgRsr7FdE2gOV1th/eWkW1tLJtVZz2m3+wuaIagP36duaBSw/lhNtfY0jPTvzq/HEZrlBERHKVAk4jmdk0oG89u65z92d297F6tjn1j4Hy3fzuFcAVAIMHD25ApZn30tw1bK6opnNxER9XxvjqMcPp2rEdr3zvODq0K6RQC2qKiEgLUcBpJHc/sQkfWwEMSnk/EFgVvd7d9rq/ex9wH8CECRPqDUHZ5pUF6xjQrQPPfeNoFq0vZ8LQHgB0KWmX4cpERCTX6S6q1jEZuMDMis1sGDASeBOYBYw0s2Fm1p4wEHlyButMm8pYnDdKNzBxv95079R+Z7gRERFpDWrBSSMzOwf4LdAbeN7M3nH3k919rpk9Rhg8HAOucvd49JmrgSlAIfCAu8/NUPlpNXPxJiqq4lp+QUREMkIBJ43c/Wng6d3suwm4qZ7tLwAvtHBprcrdeeCNJXQpKeLI4ZqhWEREWp+6qPLdR/+E3xwCa95P21dOX7SR1z5YzzdPGEmH9oVp+14REZGGUsDJd/Eq2LQIqral5evcfeeEfl84dNAnHC0iItIyFHDyXWH78ByvSsvXLVq/jcXrQ1jqrLulREQkQxRw8l2aA87CtR8DcM/F49PyfSIiIk2hgJPvCqNWljQFnLmrtmAGn963d1q+T0REpCl0F1W+S1MLzqsfrKOkqJB7Xl/M4cN6aHCxiIhklAJOvtsZcKqb9PFEwpk0azk/evq9nduu++yYdFQmIiLSZOqiynfN7KJ6fHbtcPPlo4ZqhXAREck4BZx8V1gcnpsYcD5YU17r/ZePGtrMgkRERJpPASff1XRRxZoWcNZ+vGPn69vOH8fQXp3SUZWIiEizKODku2Z2US1YvXXn68+NH5iOikRERJpNASffNeMuqq07qlmyIUzq94NTRqWzKhERkWbRXVT5rqYFp/LjRn/01QXrSDg8+Z9HMn5IjzQXJiIi0nRqwcl3BdF8Nf+8HWKVjfroE7NXMKBbBw4a1L0FChMREWk6BRxJakQ3VSye4M0lmzh5bF8KC6wFixIREWk8BRxJ8kSDD/1oYwWVsQRj+ndpwYJERESaRgFHkhLxBh9as6jmfn07t1Q1IiIiTaaAI0nuDT60dF05ZrBP771asCAREZGmUcARKI6WVvCGt+CUritnQLcOWlRTRESykgKOwEnXh+dGdFGVritX642IiGQtBRwBi1phGjjIOJFwFm8oZ0QfBRwREclOCjgCFl0GDeyimjJ3DTuqE4zSAGMREclSmslYkpP9fUIX1bqtOzjvnuks21RBz07tOXNc/1YoTkREpPHUgiMN7qKaMncNyzZVAPCN40dQ0k4DjEVEJDsp4EhKF9WeA87byzbvfN23a4eWrEhERKRZFHAECqLL4BO6qBZHK4cDHDiwa0tWJCIi0iwagyMpXVS7DzjuzpL15XzpiCHccPb+rVSYiIhI06gFR5KDjD3B9qo4//m/s1m2saLWIZu2VbF1R4xhvTploEBJi0Qcnv0WrHon05WIiLQ4BRxJjsFJxJm9tIwX31/DD56cU+uQJVH31LDeCjhtUqwKnvwqzH4Iyj7KdDUiIi1OAUdqdVGVtAuXxPJN22sdsmh9OQDDeirgtElz/gxzn4Le+8HoMzJdjYhIi1PAkZQuKmdHdbiTau3WHbUO+feyzXQpKWJwj46tXZ00V7wa3nogvL7sxeT/3iIiOUwBR8AsPCfibK8OA43rriv+1tIyxg/pTkGBtW5t0jTvPwk39oXNy+HFa2D1HDjuR9CxR6YrExFpFbqLSmp1UdUEnFSbK6ooXVfOOQcPaOXCpMlevgFi2+GO6I63kZ+BT/8gszWJiLQiteBIrbuodlTtGnAWrg3jb/YfoLlvss62DTDpItiyEqq2gTskEslWOYCug+Gce2tvExHJcWrBkVp3Ue2I7RpwyiqqAOjZqX1rViWJOJS+DMOOhWXTobgLbFoMK2fDxB9BSRd4+2FY8Fx4AJR0gx3RjNNn3QUHXRRmqNa4GxHJMwo4UruLKmrBcU+OwtlSUQ1At47tWr20vBKPwRNfhoO/FLqU/nQOLHkdOvWBbevCMQXtIFENM++GfgdBPIRPikogtiOEm/Z7wfH/DeO+GFptTOFGRPKPAo6krCae2DkGJ+FQURVjZdl2PtoY5sDp1lEtOE1WvR1evQnWzYejvwN/+SJ86WkYOD50K73zZ1g3D+Y/Gx6f/VUINwAduiUDTqIaugyArSthdTRh3xFXwsk3w44t8MqNMObM0OojIpLHFHCk1mriqYOMx/xkSq3DOrVXS0CTJOJw69DQwgJQOi083398CDIvXRuCS6oXvheer10RWmQ+Xg2v3gyDDoOx58LWVVC5FZb8HcZdEFpqOnSD037Van+WiEg2U8CR5OBTj++yREPtwzRItUkWvpQMN3XVBJkaFz4O7TvCQ6dB96FQ3Dls79Ifzvpd8rje+4bngRPSXq6ISC5QwJGdXVSJeIyp89bW2nXgwK6cOa4/m7ZVZaKy7DP1pzDwUBh9esOOX/w6TLowjKP50tOhG6lLvzAvzeyHoGwpXPJ/0LEXFO+V/NyFj0HvUS3yJ4iI5AMFHKEybhQDT81eTiwxpNa+fl1L+OoxwzNTWLZZORveuCO8vnIG9Bn9yZ959Sbo3B8+/zD0TVmFvcdwGHtOGH9TX8vYvienp2YRkTyleXCEmqlvXpm/GoBLjxzCcaN6c8zIXnzj+JEZrCzLzLgn+fr3R4TxL7tTsQk2LYHlM+Hwr4WxM/VRt5+ISItQC44QJ/xLtpCwDtWwXp342Vn77+kj+Wfpv+C9x2DchTD8OHj6CnjzDzD0mF1DSuXH8Ithyfdjz27NSkVEBLXgCBCPLgOLVqBqX6S7pWqp3gEvfD+8PvhiGPcFOPJqmD8ZZty96/GlLydf9xkbBguLiEirUguOEPPaLTjti9po7k3EIVYZ7kLanVdvhsJ2cOz36//8G3dCxcZwG3ZJFzjjTnj9Flj7Ppz5Oxj6qXDsZ26EpW/AlGvD7drH/TD5PcvfDM9HXh0m7RMRkVangCPEPQSaQmuDAadmkG48Bn+YCNvWw5CjQsgYcxYsegX2Oy2EFxz++evwuXEXQtc6i4cunAIv/6z2tm5D4F+/hTFnwyEpYcUMPntbmMvmtZ+H37lkMrQrgVX/hgET4OSbWvRPFxGR3VPAEWLRGJyCmhacwiwLOCtmw8IXYeJ1tce7LJsJj54Hn38krKe15t2w/f0nw/P0aN6YdfN2/c5fj4FrloYZhqf9NKzxBNCxJ1w5E7ZvgrsOSwaeo76x63cMHA9Xzw63gS+fCe88GoLUsn/BsVq5W0QkkxRwhHjINRREY3CK09mC8/L/wKz74eKnmjYpXawqtJJAWDhyw4cwYDwUFoXQU7kV/pQyiHfkZ8Ig3/MfgtuieWSGHgMf/QMO/AKcfgfc3C9sv7X2LfFAmCV4r97hUeOK16H/QfXX12sEXDUT7hwXFr5cPQcGHQ5HXtn4v1VERNJGAUeI1XRRpXsMzuZl8I/bwuvHLoXvzIVtG2Hte7BiFhzzvT3fJl2+Dmbem3z/r9/CW39Mvi+ILl8rBI+HcHL+g8n9X5kSZgDuNrj2fDOXPAOPnAXDPh1abDYsDCtur5tXew2nq2aFmYS79Nvz32kGg4+EdyeF96feCh267/kzIiLSohRwhFh0F1VNF1W75nRRuUMiFsLH1J8kt1eVh9aYX6ZMGvjKjXDY1+DAz+/aurP6Xbj3mPD6wAtg8Wu1ww2E37nwsdBqU7UtGXhqDD4i+To1SA0/Dq7fUvvYt/8Ek6+G4Z9ObqtZDqEh+uyXfL23brEXEcm0LBtsIZlQ04JTsPM28SZeFlXb4Ndj4YZe8OfPw9ynw51Ep/4CdmyGGXft+pk374X7T4D3nkhu274ZHrskvO7QA069BUafEd7veypc/CSMPhOO+mYIN2ZhmYN2JU2rG8Lt3z9cFmYYboq+B4bn9nuFu7RERCSj1IKTRmb2S+AMoApYBFzm7pujfdcClwNx4JvuPiXafgpwJ1AI3O/ut7R23fGQa5JdVI1twXEP88TM+kNy24d/C+svHf/foQsIYNr1IUBcOSMMxq3YED638KVwJ9IB54XveuYq2LIcvvK3sO5TQUG4I2mfieHupM57w4gTm/+HpzKDkq5N//w+x8PhXw9dVSIiknEKOOk1FbjW3WNmditwLXCNmY0BLgDGAv2BaWZW0/9xF3ASsAKYZWaT3b2e235aTixR5y6qhrbgrJ4TBhFvXVX/nUr7fy60qvQ9IIyD2bwMDv0qFBWH/e0Hw4V/DTMCv/A9uL5ruLV7wXNw8s0w+PDkdxUVh9u9s5VZGHsjIiJZQQEnjdz9bylvZwDnRa/PAia5eyWwxMxKgZrFiUrdfTGAmU2Kjm3VgFNdZwxOg++ievjM0PWU6rwHYcQJYb6ZY74btpnBRU+EWX/rm/jukEtCF9XyGTDvGeh/MByhu5BERKTpFHBazleAv0avBxACT40V0TaA5XW2pzRbJJnZFcAVAIMHD05roTUtOIUNvU182wb440m1w83lU8OdSDUDe0+8vvZneo+CM+6o//uKiuHLz8OCZ+H9p2Dij7QIpYiINIsCTiOZ2TSgbz27rnP3Z6JjrgNiwKM1H6vneKf+Qd5e3++6+33AfQATJkyo95imikXfVtOC061j+z1/4J0/JyfGO/CCsJxBcwb4QpjXZuw54SEiItJMCjiN5O57HN1qZpcCpwMnuHtNEFkBDEo5bCCwKnq9u+2tptprd1F94hicdfOTr0ed0vxwIyIikma6TTyNojuirgHOdPeKlF2TgQvMrNjMhgEjgTeBWcBIMxtmZu0JA5Ent3bd8Zouqmgtqk+0fkHydc8RLVCRiIhI86gFJ71+BxQDUy2MIZnh7l9397lm9hhh8HAMuMrd4wBmdjUwhXCb+APuPre1i652SLhRQIKSdp+QeUtfhlVvJ9/3GduyxYmIiDSBAk4auftumzPc/SZgl+Wl3f0F4IWWrOuTxBMJ4hRQSIIhPTrt/sDq7WHphe7D4PTbIZEIc9SIiIhkGQUcIRZ3EhRQgHPlxH3qPygRh5uisdWHXBomthMREclSCjhCPOEkMD5/SH96HjSg/oO2pox97jWydQoTERFpIvUvCLGEE6eADntaQqnso+Tr1BW3RUREspBacIRYPEGCAorqm4LnH7eHSfoqNoX333wHegxr3QJFREQaSS04eW7eqq1c/+w8EhjmCaj8GCZdlGyxeflnMOlCKFsCVghdB+3x+0RERLKBAk6e21xRBUCcAgo8BrMfDotdvnYrVG1LHlj2EXQbFGYcFhERyXL6t1We67lXWNk7QQHt/v1QckeH7rB1dfL9+oXQfWir1iYiItJUasHJcz06hXWnEnWXyyrpCltXJt+vfQ96j27FykRERJpOASfPde8Ybp2K170UYjtq3xoOsLdmLRYRkbZBASfPFRWGSyBR91KoKq/dggMw+IhWqkpERKR5NAZH+O/Tx9BzegmkjCmmsjzMXtyhBxz2H7CxVBP8iYhIm6GAI1x+9DB4u32dgLMVKjZClwEw8UcZq01ERKQpFHAksMLa77eXwbb1YZI/ERGRNkZjcCQoqBNwlv4LNiyEHsMzU4+IiEgzKOBIYHUvhWjZBs19IyIibZACjgQ1XVQDD4OLnkxu79QnM/WIiIg0gwKOBAXRpdB7FHTeO7m9U6/M1CMiItIMCjgS1HRRFZVAcZfk9o49M1OPiIhIMyjgSFDTRVVUHJZpqKGAIyIibZACjgQ1LTjtOkBx5+T2km6ZqUdERKQZFHAk8ER4Liqufct4gS4RERFpezTRnwTxqvBc1CE8X/aSbhEXEZE2SwFHgkQsPBcVh+chR2auFhERkWZS/4MEO1twSjJbh4iISBoo4EhQE3DadchsHSIiImmggCNBvE4XlYiISBumgCOBuqhERCSHKOBIULk1PGveGxERyQEKOBLUtOB00szFIiLS9ingSG0dtbimiIi0fQo4UlvqMg0iIiJtlAKO1GaW6QpERESaTTMZS/CVKbB5WaarEBERSQsFHAkGHxEeIiIiOUBdVCIiIpJzFHBEREQk5yjgiIiISM5RwBEREZGco4AjIiIiOUcBR0RERHKOAo6IiIjkHAUcERERyTkKOCIiIpJzFHBEREQk5yjgiIiISM5RwBEREZGco4AjIiIiOUcBR0RERHKOAo6IiIjkHAUcERERyTkKOCIiIpJzFHDSyMxuMLN3zewdM/ubmfWPtpuZ/cbMSqP9h6R85lIz+zB6XJq56kVERHKHAk56/dLdD3T3g4DngJ9E208FRkaPK4C7AcysB/BT4HDgMOCnZta91asWERHJMQo4aeTuW1PedgI8en0W8IgHM4BuZtYPOBmY6u6b3L0MmAqc0qpFi4iI5KCiTBeQa8zsJuASYAswMdo8AFiectiKaNvuttf3vVcQWn8Ays3sgzSW3QvYkMbvy2c6l+mh85g+OpfpofOYHi1xHofUt1EBp5HMbBrQt55d17n7M+5+HXCdmV0LXE3ogrJ6jvc9bN91o/t9wH1Nq3rPzOwtd5/QEt+db3Qu00PnMX10LtND5zE9WvM8KuA0kruf2MBD/ww8Twg4K4BBKfsGAqui7cfV2f5as4sUERHJcxqDk0ZmNjLl7ZnAguj1ZOCS6G6qI4At7r4amAJ8xsy6R4OLPxNtExERkWZQC0563WJmo4AEsBT4erT9BeCzQClQAVwG4O6bzOwGYFZ03P+4+6bWLRlooa6vPKVzmR46j+mjc5keOo/p0Wrn0dzrHfIhIiIi0mapi0pERERyjgKOiIiI5BwFnDxnZqeY2QfRMhI/zHQ92czMBpnZq2Y238zmmtm3ou09zGxqtNzG1JrZqPe0RIeAmRWa2b/N7Lno/TAzmxmdx7+aWftoe3H0vjTaPzSTdWcbM+tmZk+Y2YLo2jxS12Tjmdm3o3+u3zezv5hZia7JhjGzB8xsnZm9n7Kt0ddgupcuUsDJY2ZWCNxFWEpiDPBFMxuT2aqyWgz4rruPBo4ArorO1w+Bl919JPBy9B52s0SH7PQtYH7K+1uBX0fnsQy4PNp+OVDm7iOAX0fHSdKdwEvuvh8wjnBOdU02gpkNAL4JTHD3/YFC4AJ0TTbUQ+w6C3+jrsGWWLpIASe/HQaUuvtid68CJhGWlZB6uPtqd387ev0x4V8kAwjn7OHosIeBs6PXu1uiI++Z2UDgNOD+6L0BxwNPRIfUPY815/cJ4ITo+LxnZl2AY4E/Arh7lbtvRtdkUxQBHcysCOgIrEbXZIO4+9+BuncAN/YaTPvSRQo4+a3BS0VIbVGT9MHATGDvaF4jouc+0WE6v7t3B/ADwpQKAD2Bze4ei96nnqud5zHavyU6XmA4sB54MOruu9/MOqFrslHcfSXwK2AZIdhsAWaja7I5GnsNpv3aVMDJbw1eKkKSzGwv4Engv+ossLrLofVsy/vza2anA+vcfXbq5noO9Qbsy3dFwCHA3e5+MLCNZFdAfXQu6xF1hZwFDAP6ExZLPrWeQ3VNNl+zly5qKAWc/La7JSRkN8ysHSHcPOruT0Wb19Y080fP66LtOr/1+xRwppl9ROgWPZ7QotMt6h6A2udq53mM9ndl1+bwfLUCWOHuM6P3TxACj67JxjkRWOLu6929GngKOApdk83R2Gsw7demAk5+mwWMjO4UaE8YVDc5wzVlraiP/Y/AfHe/PWXXZKBmxP+lwDMp2+tboiOvufu17j7Q3YcSrrlX3P0i4FXgvOiwuuex5vyeFx2v/1oG3H0NsNzCDOoAJwDz0DXZWMuAI8ysY/TPec151DXZdI29BtO/dJG765HHD8ISEguBRYQV0TNeU7Y+gKMJTabvAu9Ej88S+t5fBj6MnntExxvhLrVFwHuEOzQy/ndk04Ow2Oxz0evhwJuEJU0eB4qj7SXR+9Jo//BM151ND+Ag4K3ouvw/oLuuySadx58R1g98H/gTUKxrssHn7i+EsUvVhJaYy5tyDQJfic5pKXBZc+vSUg0iIiKSc9RFJSIiIjlHAUdERERyjgKOiIiI5BwFHBEREck5CjgiIiKScxRwRCTvmZmb2XmffGSTv39C9BtDW+o3RKQ2BRwRadPM7KEoPNR9zGjE1/QDnm2pGkWk9RV98iEiIllvGvClOtuqGvphDzMCi0gOUQuOiOSCSndfU+exCXZ2P11tZs+bWYWZLTWzi1M/XLeLysx+Eh1XaWZrzOyRlH3FZnaHma01sx1mNsPMjq7zfaeY2YJo/z+AfesWbGZHmdnrUU0rzexuM+uSsv/Y6LvLzWyLmc00s/3TeM5EcpoCjojkg58R1sA5CLgPeMTMJtR3oJl9DvgecCUwEjidMB1/jV8AXyBMK38wYbr5l1IWFhxEWDJhavR7v40+k/obBwB/i2oaB5wbHftAtL+IsHbPP6P9hwN3AvGmnwKR/KKlGkSkTTOzh4CLgR11dt3l7teYmQP3u/t/pHxmGrDG3S+O3jtwvrs/YWbfAb4G7O9hZenU3+oElAFfdfdHom2FhPXc/uLuPzazmwkLMI7ymsV3zH4M3AAMc/ePohahane/POW7DwL+DewNxICNwHHu/noaTpNI3tEYHBHJBX8HrqizbXPK6+l19k0HTtvNdz0OfAtYYmZTgJeAye5eCewDtAPeqDnY3eNmNh0YE20aDczw2v/1WPf3xwMjzOwLKdsset7H3adHwW2Kmb1MWKzwcXdfvpuaRaQOdVGJSC6ocPfSOo8NTfmiKESMIrTibAVuA2ZHrTc1IaS+pu+abVbPvroKgPsJ3VI1j3GELrF3ojouI3RN/R04E1hoZic34U8SyUsKOCKSD46o5/383R3s7jvc/Xl3/zZwKDAW+BRQSrg7a+eg4qiL6khgXrRpHnC4maUGnbq//zYwtp5QVuru21PqmOPut7r7ccBrwKUN/otF8py6qEQkFxSbWd862+Luvj56fa6ZzSKEhPOAEwitI7swsy8T/r9xJlBOGFBcDXzo7tvM7G7gFjPbACwBvk0YN/P76CvuAb4L3GFmvwcOAL5e52duBWaY2T3AvcDHwH7AGe7+NTMbRmhBmgysBIYDBwJ3N+akiOQzBRwRyQUnAqvrbFsJDIxeXw98DvgNsB64zN1n7ea7NgPXAL8ijLeZB5zr7kui/ddEzw8C3QgDg09x99UA7r7MzM4FbieElNnAD4H/rfkBd38zBOhaAAAAb0lEQVTXzI4FbgReBwqBxcDT0SEVhFvLHwd6AWuBRwnBSEQaQHdRiUhOS71DKtO1iEjr0RgcERERyTkKOCIiIpJz1EUlIiIiOUctOCIiIpJzFHBEREQk5yjgiIiISM5RwBEREZGco4AjIiIiOef/AclNO9Opary+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\", \"random_agent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b55ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53edd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
